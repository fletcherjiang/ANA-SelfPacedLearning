{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiyang/anaconda3/envs/mamba/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt2/yiyang/ANA/ana_new/utils_single.py:368: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  indices = list(range(20)) if 20 is not None else list(range(len(train_dataset)))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "from config_single import *  # Import everything from config.py\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch.utils.data\n",
    "import scipy.misc as misc\n",
    "import torch.optim as optim\n",
    "import sklearn.metrics as sm\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import transforms as transforms\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#自定义函数\n",
    "from data_weighted_single import get_traindata as get_traindata_single, get_testdata as get_testdata_single\n",
    "from data_weighted_multiple_filename import get_traindata as get_traindata_multiple, get_testdata as get_testdata_multiple\n",
    "from data_weighted_filename import get_traindata, get_testdata\n",
    "\n",
    "from utils_single import set_random_seed, Logger, inference, get_small, train, test, save, get_images, test_small, get_train_dataset, get_train_dataloader,save_results_to_json\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "def cv_imread(file_path):\n",
    "    # 避免中文路径imread无法使用，重写了函数\n",
    "    # Open source version - reads standard image files without decryption\n",
    "    cv_img = cv2.imdecode(np.fromfile(file_path, dtype=np.uint8), -1)\n",
    "    return cv_img\n",
    "\n",
    "set_random_seed(42 , True)\n",
    "# os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"2\")\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--granularity'], dest='granularity', nargs=None, const=None, default='label', type=<class 'str'>, choices=None, required=False, help='granularity of weights', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv = [arg for arg in sys.argv if not arg.startswith('--f=')]\n",
    "\n",
    "parser = argparse.ArgumentParser(description='HQnet-for-resnet-pre-train')\n",
    "parser.add_argument('--lr',              default=LEARNING_RATE,     type=float, help='learning rate')\n",
    "parser.add_argument('--epoch',           default=EPOCH,             type=int,   help='number of epochs')\n",
    "parser.add_argument('--trainBatchSize',  default=BATCH_SIZE,        type=int,   help='training batch size')\n",
    "parser.add_argument('--testBatchSize',   default=BATCH_SIZE,        type=int,   help='testing batch size')\n",
    "parser.add_argument('--weightDecay',     default=WEIGHT_DECAY,      type=float, help='weight decay')\n",
    "parser.add_argument('--pathModelParams', default=None, type=str, help='path of model parameters')\n",
    "parser.add_argument('--saveModel',       default=True, action='store_true', help='save model parameters')\n",
    "parser.add_argument('--loadModel',       action='store_true', help='load model parameters')\n",
    "parser.add_argument('--weight_lr', default=LEARNING_RATE, type=float, help='learning rate of weights')\n",
    "parser.add_argument('--initWeight', default='iw-ones', type=str, help='initilization method of weights')\n",
    "parser.add_argument('--updateLR', default='ulr-ones', type=str, help='updating method of learning rates')\n",
    "parser.add_argument('--trainingLabel', default='real', type=str, help='type of training labels')\n",
    "parser.add_argument('--sampling', action='store_true', help='weighted sampling')\n",
    "parser.add_argument('--granularity', default='label', type=str, help='granularity of weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetch the key successfully!\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args()\n",
    "# No encryption in open source version\n",
    "print('Running open source version without encryption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:27: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:27: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt2/yiyang/ANA/ana_new/all_one.csv\n",
      "Data Preparation : Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_179319/4180727348.py:27: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  val_indices = list(range(20)) if 20 is not None else list(range(len(val_dataset)))\n",
      "/home/yiyang/anaconda3/envs/mamba/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/yiyang/anaconda3/envs/mamba/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Preparation : Finished\n",
      "Initialize weights\n",
      "tensor([[0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "Initialize weights\n",
      "label_types tensor([[0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64) tensor(1., dtype=torch.float64)\n",
      "learning_rates tensor([1.])\n",
      "Initialization Done 43.25738549232483\n",
      "label_types tensor([[0., 0., 0., 1., 0., 0., 0., 0.]], dtype=torch.float64) tensor(1., dtype=torch.float64)\n",
      "learning_rates tensor([1.])\n",
      "Initialization Done 72.45792508125305\n"
     ]
    }
   ],
   "source": [
    "if args.pathModelParams is None:\n",
    "    PATH_MODEL_PARAMS  = '/mnt2/yiyang/ANA/ana_new/new_2.25_single_1_' +'iw_al_val_' + str(args.lr) + 'lr_' + str(args.weight_lr) + 'weight_' + args.initWeight + '_' + args.updateLR + '_trainingLabel_' + str(args.trainingLabel) + '_sampling_' + str(args.sampling) + '_granularity_' + args.granularity\n",
    "else:\n",
    "    PATH_MODEL_PARAMS = args.pathModelParams\n",
    "    \n",
    "if args.saveModel:\n",
    "    os.makedirs(PATH_MODEL_PARAMS, exist_ok = True)\n",
    "\n",
    "sys.stdout = Logger(f\"{PATH_MODEL_PARAMS}/output.txt\")\n",
    "# ==================================================================\n",
    "# 准备数据\n",
    "# ==================================================================\n",
    "normalize = transforms.Normalize(mean=[0.005, 0.190, 0.006],\n",
    "                                  std=[0.008, 0.102, 0.008]) #计算得出\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "                    transforms.Resize((N, N)), \n",
    "                    transforms.ToTensor(),\n",
    "                    normalize\n",
    "                  ]) \n",
    "val_dataset = get_testdata(root=DIR_TEST_IMAGES,\n",
    "                            annFile=PATH_TEST_ANNFILE, \n",
    "                            transform=val_transforms,\n",
    "                          split = 'val')\n",
    "\n",
    "print(PATH_TEST_ANNFILE)\n",
    "val_indices = list(range(20)) if 20 is not None else list(range(len(val_dataset)))\n",
    "val_dataset = Subset(val_dataset, val_indices)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                            batch_size=args.testBatchSize,  \n",
    "                            shuffle=False,\n",
    "                            num_workers = 4)\n",
    "\n",
    "print('Data Preparation : Finished')\n",
    "\n",
    "resnet50 = torchvision.models.resnet50(pretrained=True) #使用预训练\n",
    "resnet50.fc = nn.Linear(2048, NUM_CATEGORIES)\n",
    "\n",
    "if GPU_IN_USE:\n",
    "    resnet50.to(device)\n",
    "\n",
    "print('Model Preparation : Finished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================================================================\n",
    "# loss function\n",
    "# ==================================================================\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightedBCELoss, self).__init__()\n",
    "    def forward(self, input, target, weight):\n",
    "        bce = F.binary_cross_entropy(input, target, reduction = 'none')        \n",
    "        if args.granularity == 'sample':\n",
    "            bce *= F.relu(weight.nan_to_num())[:, None]\n",
    "        else:\n",
    "            bce[target == 1] *= F.relu(weight[target == 1].nan_to_num())\n",
    "        \n",
    "        return bce.sum()\n",
    "    \n",
    "loss_function = WeightedBCELoss()\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# Main Loop\n",
    "# ==================================================================\n",
    "t_start = time.time()\n",
    "train_dataset = get_train_dataset()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                batch_size=args.trainBatchSize, \n",
    "                                num_workers = 4)\n",
    "\n",
    "print('Initialize weights')\n",
    "labels = torch.zeros((len(train_dataset), NUM_CATEGORIES), dtype = torch.float64)\n",
    "label_filenames = np.array([''] * len(train_dataset)).astype('U256')\n",
    "weight_idx = []\n",
    "label_types = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Data: 100%|██████████| 1/1 [00:00<00:00,  5.91batch/s]\n"
     ]
    }
   ],
   "source": [
    "for batch_num, (_, label, idx, filenames) in tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Processing Train Data\", unit=\"batch\"):\n",
    "    labels[idx] = label\n",
    "    label_filenames[idx] = filenames\n",
    "\n",
    "    for i, l1 in enumerate(label):\n",
    "        break_flag = False\n",
    "        for j, l2 in enumerate(label_types):\n",
    "            if (l1 - l2).sum() == 0:\n",
    "                break_flag = True\n",
    "                weight_idx[j].append(idx[i])\n",
    "                break\n",
    "        if not break_flag:\n",
    "            label_types.append(l1)\n",
    "            weight_idx.append([idx[i]])\n",
    "\n",
    "            \n",
    "print('label_types', torch.stack(label_types), torch.stack(label_types).sum(dim = 1).max())\n",
    "\n",
    "targets = labels.clone()\n",
    "\n",
    "if args.updateLR == 'ulr-ones':\n",
    "    learning_rates = torch.ones(len(label_types))\n",
    "else:\n",
    "    learning_rates = (torch.log(torch.stack(label_types).sum(dim=1)) / torch.log(torch.stack(label_types).sum(dim = 1).max())).cuda()\n",
    "print('learning_rates', learning_rates)\n",
    "weight_idx_flatten = torch.tensor([ww for w in weight_idx for ww in w])\n",
    "group_idx = torch.argsort(weight_idx_flatten)\n",
    "\n",
    "\n",
    "\n",
    "if args.initWeight == 'iw-ones':\n",
    "    #Optimized version #1\n",
    "    if args.granularity == 'sample':\n",
    "        weights = [torch.ones(len(train_dataset))[ sum(list(map(len, weight_idx))[:i]) : sum(list(map(len, weight_idx))[:i]) + len(weight_idx[i])] for i in range(len(weight_idx))]\n",
    "    else:\n",
    "        weights = [labels[weight_idx_flatten][ sum(list(map(len, weight_idx))[:i]) : sum(list(map(len, weight_idx))[:i]) + len(weight_idx[i])] for i in range(len(weight_idx))]\n",
    "elif args.initWeight == 'iw-data':\n",
    "    #Optimized version #2\n",
    "    if args.granularity == 'sample':\n",
    "        iw = F.softmax(torch.tensor(list(map(len, weight_idx))) / len(train_dataset))\n",
    "        weights = [torch.ones(len(weight_idx[i])) * iw[i] for i in range(len(weight_idx))]\n",
    "    else:\n",
    "        iw = labels.sum(dim = 0)\n",
    "        iw /= iw.sum()\n",
    "    \n",
    "        iw = labels * iw\n",
    "        iw[labels == 0] = float('-inf')\n",
    "    \n",
    "        iw = F.softmax(iw) #.float()\n",
    "        \n",
    "        weights = [(labels * iw)[weight_idx_flatten][ sum(list(map(len, weight_idx))[:i]) : sum(list(map(len, weight_idx))[:i]) + len(weight_idx[i])] for i in range(len(weight_idx))]\n",
    "\n",
    "elif args.initWeight == 'iw-sample':\n",
    "    #Optimized version #3\n",
    "    iw = torch.zeros((len(train_dataset), NUM_CATEGORIES), dtype = torch.float64)\n",
    "    for filename in np.unique(label_filenames):\n",
    "        iw_sub = labels[label_filenames == filename].sum(dim = 0)\n",
    "        iw_sub /= iw_sub.sum()\n",
    "        iw[label_filenames == filename] = iw_sub\n",
    "    #iw = iw.float()\n",
    "    weights = [iw[weight_idx_flatten][ sum(list(map(len, weight_idx))[:i]) : sum(list(map(len, weight_idx))[:i]) + len(weight_idx[i])] for i in range(len(weight_idx))]\n",
    "else:\n",
    "    print('No valid initilization method for weights')\n",
    "    exit()\n",
    "\n",
    "\n",
    "\n",
    "print('Initialization Done', time.time() - t_start)\n",
    "    \n",
    "weight_lr = args.weight_lr\n",
    "weight_params = []\n",
    "for p, l in zip(weights, learning_rates):\n",
    "    p.requires_grad = True\n",
    "    def print_grad(grad):\n",
    "        if torch.isnan(grad).sum() > 0:\n",
    "            print(torch.isnan(grad).sum())\n",
    "    p.register_hook(print_grad)\n",
    "    weight_params.append({'params': p, 'lr': l * weight_lr})\n",
    "optimizer = optim.Adam([{'params': resnet50.parameters()}] + weight_params, lr=args.lr, weight_decay=args.weightDecay, betas=MOMENTUM)\n",
    "\n",
    "num_early_stop = 5\n",
    "count_early_stop = 0\n",
    "count_early_stop_single = 0\n",
    "count_early_stop_multiple = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 1., 0., 0., 0., 0.], dtype=torch.float64,\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_epoch in range(1, args.epoch + 1):\n",
    "    print('\\n===> epoch: %d/%d' % (current_epoch, args.epoch))\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     train_loader = get_train_dataloader(resnet50, torch.cat(weights)[group_idx], labels, train_dataset, args)\n",
    "    \n",
    "    train_cp, train_cr, train_cf1, train_op, train_or, train_of1 = train(optimizer, train_loader, weights, args, resnet50, group_idx, loss_function,targets,device)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        #'''\n",
    "        if args.updateLR == 'ulr-adaptive':\n",
    "            #Update weights of learning rates\n",
    "            weight_idx = []\n",
    "            label_types = []\n",
    "            for i, l1 in enumerate((targets > 0).float()):\n",
    "                break_flag = False\n",
    "                for j, l2 in enumerate(label_types):\n",
    "                    if (l1 - l2).sum() == 0:\n",
    "                        break_flag = True\n",
    "                        weight_idx[j].append(i)\n",
    "                        break\n",
    "                if not break_flag:                \n",
    "                    label_types.append(l1)\n",
    "                    weight_idx.append([i])\n",
    "            print('label_types', torch.stack(label_types), torch.stack(label_types).sum(dim = 1).max())\n",
    "            #weights = [torch.zeros(len(idx), NUM_CATEGORIES) for idx in weight_idx]\n",
    "            weight_idx_flatten = torch.tensor([ww for w in weight_idx for ww in w])\n",
    "            weights = [torch.cat(weights)[group_idx][weight_idx_flatten][ sum(list(map(len, weight_idx))[:i]) : sum(list(map(len, weight_idx))[:i]) + len(weight_idx[i])] for i in range(len(weight_idx))]\n",
    "\n",
    "            learning_rates = (torch.log(torch.stack(label_types).sum(dim=1)) / torch.log(torch.stack(label_types).sum(dim = 1).max())).cuda()\n",
    "            learning_rates[torch.isinf(learning_rates)] = 0\n",
    "            learning_rates[torch.isnan(learning_rates)] = 0\n",
    "            print('learning_rates', learning_rates)\n",
    "            #weight_idx_flatten = torch.tensor([ww for w in weight_idx for ww in w])\n",
    "            group_idx = torch.argsort(weight_idx_flatten)\n",
    "\n",
    "            weight_params = []\n",
    "            for p, l in zip(weights, learning_rates):\n",
    "                p.requires_grad = True\n",
    "                weight_params.append({'params': p, 'lr': l * weight_lr})\n",
    "            optimizer = optim.Adam([{'params': resnet50.parameters()}] + weight_params, lr=args.lr, weight_decay=args.weightDecay, betas=MOMENTUM)\n",
    "        #'''\n",
    "        \n",
    "        acc_total, f1_mi, f1_ma, mAP = test_small(val_loader,resnet50,device)\n",
    "\n",
    "    if f1_ma > of1 and f1_mi > cf1:\n",
    "        of1 = f1_ma\n",
    "        cf1 = f1_mi    \n",
    "        \n",
    "    if f1_mi > best_mi:\n",
    "        if args.saveModel:\n",
    "            save(f'{PATH_MODEL_PARAMS}/{current_epoch}_f1mi.pt',resnet50)\n",
    "        best_mi = f1_mi\n",
    "        count_early_stop = 0\n",
    "    if f1_ma > best_ma:\n",
    "        if args.saveModel:\n",
    "            save(f'{PATH_MODEL_PARAMS}/{current_epoch}_f1ma.pt',resnet50)\n",
    "        best_ma = f1_ma\n",
    "        count_early_stop = 0\n",
    "        \n",
    "    if acc_total > best_acc:\n",
    "        if args.saveModel:\n",
    "            save(f'{PATH_MODEL_PARAMS}/{current_epoch}_acc.pt',resnet50)\n",
    "        best_acc = acc_total\n",
    "        count_early_stop = 0\n",
    "        \n",
    "    if mAP > best_mAP:\n",
    "        if args.saveModel:\n",
    "            save(f'{PATH_MODEL_PARAMS}/{current_epoch}_mAP.pt',resnet50)\n",
    "        best_mAP = mAP\n",
    "        count_early_stop = 0\n",
    "        \n",
    "    count_early_stop += 1\n",
    "    if count_early_stop >= num_early_stop:\n",
    "        break\n",
    "  \n",
    "    val_results = {\n",
    "    'acc_total': acc_total,\n",
    "    'f1_mi': f1_mi,\n",
    "    'f1_ma': f1_ma,\n",
    "    'mAP': mAP,\n",
    "    }\n",
    "\n",
    "    print('===> BEST PERFORMANCE (OF1/CF1): %.3f / %.3f' % (of1, cf1))\n",
    "    print('===> BEST PERFORMANCE (mi/ma): %.3f / %.3f' % (best_mi, best_ma))\n",
    "    print('===> BEST PERFORMANCE (acc): %.3f' % (best_acc))\n",
    "    print('===> BEST PERFORMANCE (mAP): %.3f' % (best_mAP))\n",
    "\n",
    "    \n",
    "    if count_early_stop == 1:\n",
    "        with torch.no_grad():\n",
    "            acc_0, acc_1, acc_2, acc_3, acc_4, acc_5, acc_6, acc_7, acc_total, f1_mi, f1_ma, mAP = test(resnet50,args,device)\n",
    "            print('All', 'Acc.', acc_total, 'F1_Mi', f1_mi, 'F1_Ma', f1_ma, 'mAP', mAP)\n",
    "\n",
    "            test_results = {\n",
    "                        'acc_total': acc_total,\n",
    "                        'f1_mi': f1_mi,\n",
    "                        'f1_ma': f1_ma,\n",
    "                        'mAP': mAP,\n",
    "                    }\n",
    "            save_results_to_json(current_epoch, val_results, test_results, f\"{PATH_MODEL_PARAMS}/results.json\")\n",
    "\n",
    "    t_end = time.time()\n",
    "    t_use = int((t_end - t_start)/60)\n",
    "    print(f'Use {t_use}/min ')\n",
    "\n",
    "print(PATH_MODEL_PARAMS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
